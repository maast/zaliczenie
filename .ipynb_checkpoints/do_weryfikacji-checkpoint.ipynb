{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 15 11:31:52 2015\n",
    "\n",
    "@author: bolaka\n",
    "\n",
    "This solution measures 0.84020 AUC on private leaderboard. (alas I did not submit it!)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "# suppress pandas warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "\n",
    "# imports\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "from mlclassificationlibs import * \n",
    "from numpy.random import seed\n",
    "from scipy.special import cbrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# reproduce results\n",
    "seed(786)\n",
    "\n",
    "# load the training and test sets\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d-%b-%y')\n",
    "data = pd.read_csv('Train.csv', index_col = 'ID', encoding = 'latin1', parse_dates = ['Lead_Creation_Date'], date_parser=dateparse) # ,'DOB'\n",
    "test=pd.read_csv('Test.csv', index_col = 'ID', parse_dates = ['Lead_Creation_Date'], encoding = 'latin1', date_parser=dateparse) # ,'DOB'\n",
    "\n",
    "# sum up Missing\n",
    "def sumMissing(s):\n",
    "    return s.isnull().sum()\n",
    "\n",
    "# fix the format of DOB & convert both Lead_Creation_Date & DOB to int\n",
    "def fixDates(data):\n",
    "    data['DOB_yr'] = [item.split('-')[2] for item in data['DOB']]\n",
    "    data.DOB_yr = '19' + data.DOB_yr\n",
    "    data['DOB_mon'] = [item.split('-')[1] for item in data['DOB']]\n",
    "    data['DOB_day'] = [item.split('-')[0] for item in data['DOB']]\n",
    "    data.DOB = data.apply(lambda x: pd.datetime.strptime(\"{0} {1} {2} 00:00:00\".format(x['DOB_yr'],x['DOB_mon'], x['DOB_day']), \"%Y %b %d %H:%M:%S\"),axis=1)\n",
    "\n",
    "    # drop extra features\n",
    "    data.drop( [ 'DOB_mon', 'DOB_day' ] , axis=1, inplace=True) # 'DOB_yr', \n",
    "    data.DOB_yr = [int(x) for x in data.DOB_yr]\n",
    "\n",
    "    # convert dates to ordinal\n",
    "    data['Lead_Creation_Date'] = data['Lead_Creation_Date'].apply(lambda x: x.toordinal())    \n",
    "    data['DOB'] = data['DOB'].apply(lambda x: x.toordinal())  \n",
    "    \n",
    "    return data\n",
    "\n",
    "def dictMap(listOfMajors, non_major):\n",
    "    mapped_dict = {}\n",
    "    for i, major in enumerate(reversed(listOfMajors)):\n",
    "        mapped_dict[major] = (i+1)\n",
    "    mapped_dict[non_major] = 0\n",
    "    return mapped_dict\n",
    "\n",
    "def dictMap0(listOfMajors):\n",
    "    mapped_dict = {}\n",
    "    for i, major in enumerate(reversed(listOfMajors)):\n",
    "        mapped_dict[major] = i\n",
    "    return mapped_dict\n",
    "\n",
    "# Pre-process data\n",
    "def dataPreprocessing(data, test):\n",
    "    # add the target columns to test data as 9999\n",
    "    test['LoggedIn'] = 9999\n",
    "    test['Disbursed'] = 9999\n",
    "    \n",
    "    # combine the training and test datasets for data preprocessing\n",
    "    combined = pd.concat( [ data, test ] )    \n",
    "    \n",
    "    combined = fixDates(combined)\n",
    "    \n",
    "    # Gender - Female = 0, Male = 1\n",
    "    combined['Gender'] = combined['Gender'].map( {'Female': 0, 'Male': 1} ).astype(int)\n",
    "    \n",
    "    # Filled_Form - N = 0, Y = 1\n",
    "    combined['Filled_Form'] = combined['Filled_Form'].map( {'N': 0, 'Y': 1} ).astype(int)\n",
    "    \n",
    "    # Device_Type - Mobile = 0, Web-browser = 1\n",
    "    combined['Device_Type'] = combined['Device_Type'].map( {'Mobile': 0, 'Web-browser': 1} ).astype(int)\n",
    "    \n",
    "    # Mobile_Verified - N = 0, Y = 1\n",
    "    combined['Mobile_Verified'] = combined['Mobile_Verified'].map( {'N': 0, 'Y': 1} ).astype(int)\n",
    "    \n",
    "    # City\n",
    "    city_counts = data.City.value_counts()\n",
    "    major_cites = city_counts.index[:11]\n",
    "    combined.loc[ ~combined['City'].isin(major_cites), 'City' ] = 'Non-major city'\n",
    "    mapped_cities = dictMap(major_cites, 'Non-major city')\n",
    "    combined['City'] = combined['City'].map( mapped_cities ).astype(int)\n",
    "    \n",
    "#     Employer_Name\n",
    "    # clean up outliers in TCS - the most occuring and most 'trusted' employer\n",
    "    data.loc[ data.Employer_Name.isin( ['TATA CONSALTANCY SERVICES', 'TATA CONSULTANCY SERVICE', 'TATA CONSULTANCY SERVICES', \n",
    "                                        'TATA CONSULTANCY SERVICES LIMITED', 'TATA CONSULTANCY SERVICES LTD (TCS)CONSUL'] ) , 'Employer_Name' ] = 'TATA CONSULTANCY SERVICES LTD (TCS)'\n",
    "    combined.loc[ combined.Employer_Name.isin( ['TATA CONSALTANCY SERVICES', 'TATA CONSULTANCY SERVICE', 'TATA CONSULTANCY SERVICES', \n",
    "                                        'TATA CONSULTANCY SERVICES LIMITED', 'TATA CONSULTANCY SERVICES LTD (TCS)CONSUL'] ) , 'Employer_Name' ] = 'TATA CONSULTANCY SERVICES LTD (TCS)'\n",
    "    # TODO - clean similar outliers in other popular employers if any\n",
    "    \n",
    "    # ranking employers by Disbursed sum\n",
    "    employer_groups = data.groupby('Employer_Name')['Disbursed'].sum()\n",
    "    major_employers = list(employer_groups.order()[-20:].index)\n",
    "    major_employers.remove('0')\n",
    "    major_employers.remove('TYPE SLOWLY FOR AUTO FILL')\n",
    "    combined.loc[ ~combined['Employer_Name'].isin(major_employers), 'Employer_Name' ] = 'Non-major employer'\n",
    "    \n",
    "    mapped_employers = dictMap(major_employers, 'Non-major employer')\n",
    "    combined['Employer_Name'] = combined['Employer_Name'].map( mapped_employers ).astype(int)\n",
    "    \n",
    "    # Salary_Account\n",
    "    bank_counts = data.Salary_Account.value_counts()\n",
    "    major_banks = list(bank_counts.index[:20])\n",
    "    combined.loc[ ~combined['Salary_Account'].isin(major_banks), 'Salary_Account' ] = 'Non-major bank'\n",
    "    mapped_banks = dictMap(major_banks, 'Non-major bank')\n",
    "    combined['Salary_Account'] = combined['Salary_Account'].map( mapped_banks ).astype(int)\n",
    "    \n",
    "    # Var1\n",
    "    var1_counts = data.Var1.value_counts()\n",
    "    major_var1 = list(var1_counts.index[:7]) # \n",
    "    combined.loc[ ~combined['Var1'].isin(major_var1), 'Var1' ] = 'Non-major var1'\n",
    "    mapped_var1 = dictMap(major_var1, 'Non-major var1')\n",
    "    combined['Var1'] = combined['Var1'].map( mapped_var1 ).astype(int)\n",
    "    \n",
    "    # Var2\n",
    "    var2_counts = data.Var2.value_counts()\n",
    "    major_var2 = list(var2_counts.index)\n",
    "    mapped_var2 = dictMap0(major_var2)\n",
    "    combined['Var2'] = combined['Var2'].map( mapped_var2 ).astype(int)\n",
    "    \n",
    "    # Source\n",
    "    source_counts = data.Source.value_counts()\n",
    "    major_source = list(source_counts.index[:7])\n",
    "    combined.loc[ ~combined['Source'].isin(major_source), 'Source' ] = 'Non-major source'\n",
    "    mapped_source = dictMap(major_source, 'Non-major source')\n",
    "    combined['Source'] = combined['Source'].map( mapped_source ).astype(int)\n",
    "\n",
    "    # Transformations to correct skewness...\n",
    "    combined.Monthly_Income = combined.Monthly_Income.apply(np.sqrt) \n",
    "    \n",
    "    combined.Loan_Amount_Applied = combined.Loan_Amount_Applied.apply(np.sqrt)\n",
    "    \n",
    "    combined.Existing_EMI = [np.power(x, (float(1)/3)) for x in combined.Existing_EMI ] # combined.Existing_EMI.apply(np.sqrt)\n",
    "    \n",
    "    combined.Loan_Amount_Submitted = combined.Loan_Amount_Submitted.apply(np.sqrt)\n",
    "    \n",
    "    combined.DOB_yr = [np.log(x + 1) for x in combined.DOB_yr ] \n",
    "    \n",
    "    combined.Processing_Fee = combined.Processing_Fee.apply(np.sqrt)\n",
    "    \n",
    "    # removing outliers    \n",
    "    combined = removeOutliers(combined)\n",
    "    \n",
    "#    # sum up missing    \n",
    "#    combined['missingness'] = combined.apply(sumMissing, 1)     \n",
    "    \n",
    "#    # fill missing\n",
    "#    combined = fillMissingby9999(combined)\n",
    "    \n",
    "    # separate again into training and test sets\n",
    "    data = combined.loc[ combined.Disbursed != 9999 ]\n",
    "    test = combined.loc[ combined.Disbursed == 9999 ]\n",
    "    \n",
    "    # remove the target columns from test data\n",
    "    test.drop(['LoggedIn','Disbursed'], axis=1, inplace=True)    \n",
    "    \n",
    "    return data, test\n",
    "\n",
    "# tolerance 10 standard deviations - replace by NaN\n",
    "def removeOutliers(data):\n",
    "    # remove outliers (replacing by null for std > outlier_cutoff)\n",
    "    outlier_cutoff = 10\n",
    "    for feature in data.columns:\n",
    "        if feature in ['LoggedIn', 'Disbursed', 'Employer_Name']:\n",
    "            continue\n",
    "        \n",
    "        data[feature + '_std'] = np.abs( (data[feature] - data[feature].mean()) / data[feature].std() )\n",
    "        if len( data.loc[ data[ feature + '_std' ] > outlier_cutoff, feature ] ) > 0:\n",
    "            print('removing outliers in ', feature, ':\\n', data.loc[ data[ feature + '_std' ] > outlier_cutoff, feature ])\n",
    "            data.loc[ data[feature + '_std'] > outlier_cutoff, feature ] = float('nan')\n",
    "        data.drop( [feature + '_std'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "# fill missing values by -9999\n",
    "def fillMissingby9999(data):\n",
    "    data.fillna(-9999,inplace=True)\n",
    "    return data\n",
    "\n",
    "# Cross validation & modeling\n",
    "def modeling(data, test, classifier):\n",
    "    print()\n",
    "    print('class % before split = ', data.Disbursed.sum(), '/', len(data.Disbursed))\n",
    "    \n",
    "    # Divide data roughly into train and unseen Validate\n",
    "    data['is_train'] = np.random.uniform(0, 1, len(data)) <= .80\n",
    "    train, validate = data[data['is_train']==True], data[data['is_train']==False]\n",
    "#    print('unseen validation set has', len(validate), 'records')\n",
    "    \n",
    "    # the feature set\n",
    "    features=[\n",
    "    #'DOB',\n",
    "    'DOB_yr', \n",
    "#    'Lead_Creation_Date',\n",
    "    'Gender',\n",
    "    'City',\n",
    "    'Monthly_Income',\n",
    "    'Loan_Amount_Applied',\n",
    "    'Loan_Tenure_Applied',\n",
    "    'Existing_EMI',\n",
    "    'Employer_Name',\n",
    "    'Salary_Account',\n",
    "    'Mobile_Verified',\n",
    "    'Var5',\n",
    "    'Var1',\n",
    "    'Loan_Amount_Submitted',\n",
    "    'Loan_Tenure_Submitted',\n",
    "    'Interest_Rate',\n",
    "    'Processing_Fee',\n",
    "    'EMI_Loan_Submitted',\n",
    "    'Filled_Form',\n",
    "    'Device_Type',\n",
    "    'Var2',\n",
    "    'Source',\n",
    "    'Var4',\n",
    "#    'missingness'\n",
    "    ]\n",
    "    \n",
    "    # X and Y\n",
    "    x = data[list(features)].values\n",
    "    y = data['Disbursed'].values\n",
    "    x_train = train[list(features)].values\n",
    "    x_validate = validate[list(features)].values\n",
    "    y_train = train['Disbursed'].values\n",
    "    y_validate = validate['Disbursed'].values\n",
    "    x_test = test[list(features)].values\n",
    "    \n",
    "#    print('class % in train = ', sum(y_train), '/', len(y_train))\n",
    "#    print('class % in validation = ', sum(y_validate), '/', len(y_validate))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Stratified 5-fold cross validation\n",
    "    number_of_folds = 5\n",
    "    skf = StratifiedKFold(y_train, n_folds=number_of_folds, shuffle=False)\n",
    "    y_pred = np.array(y_train.copy()).astype('float')\n",
    "    \n",
    "    # Iterate through folds\n",
    "    importance_cv = pd.Series(data=None, index=features)\n",
    "    fold = 0\n",
    "    cv_auc = []\n",
    "    for train_index, test_index in skf:\n",
    "        x_train_cv, x_test_cv = x_train[train_index], x_train[test_index]\n",
    "        y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        ensemble = classifier.fit(x_train_cv, y_train_cv)\n",
    "    \n",
    "        #Plot ROC_AUC curve and cross validate\n",
    "        disbursed = ensemble.predict_proba(x_test_cv)\n",
    "        disbursed1 = ensemble.predict_proba(x_train_cv)\n",
    "        \n",
    "        y_pred[test_index] = disbursed[:,1]\n",
    "        fpr, tpr, _ = roc_curve(y_test_cv, disbursed[:,1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fpr1, tpr1, _ = roc_curve(y_train_cv, disbursed1[:,1])\n",
    "        roc_auc1 = auc(fpr1, tpr1)\n",
    "        print('fold', str(fold+1),'--- train ROC_AUC = ', roc_auc1, '--- test ROC_AUC = ', roc_auc)    \n",
    "        cv_auc.append(roc_auc)\n",
    "        \n",
    "        # fold feature importance\n",
    "        mapFeat = dict(zip([\"f\"+str(i) for i in range(len(features))],features))\n",
    "        ts = pd.Series(ensemble.booster().get_fscore())\n",
    "        ts.index = ts.reset_index()['index'].map(mapFeat)\n",
    "        importance_cv = importance_cv.add(ts, fill_value=0)\n",
    "        fold += 1\n",
    "    \n",
    "    # plot the feature importance accross folds\n",
    "    importance_cv.dropna().order().plot(kind=\"barh\", title=(\"features importance accross 10-folds\")) # [-15:]\n",
    "    \n",
    "    print('mean AUC', np.average(cv_auc), 'STD AUC = ', np.std(cv_auc))\n",
    "    \n",
    "    #Plot ROC_AUC curve and cross validate\n",
    "    #disbursed = ensemble.predict_proba(x_validate)\n",
    "    fpr_cv, tpr_cv, _ = roc_curve(y_train, y_pred)\n",
    "    roc_auc_cv = auc(fpr_cv, tpr_cv)\n",
    "    print('Cross validation ROC_AUC = ', roc_auc_cv)\n",
    "    \n",
    "    # train the model on training data and test on validation set\n",
    "    train_ensemble = classifier.fit(x_train, y_train)\n",
    "    # TODO try this out later\n",
    "    #train_ensemble = xgb.XGBClassifier(missing=float('nan')).fit(x_train, y_train, eval_set = [(x_validate, y_validate)], eval_metric=\"auc\", early_stopping_rounds=10)\n",
    "    train_disbursed = train_ensemble.predict_proba(x_validate)\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_validate, train_disbursed[:,1])\n",
    "    roc_auc_val = auc(fpr_val, tpr_val)\n",
    "    print('Unseen validation ROC_AUC = ', roc_auc_val)\n",
    "\n",
    "    # train the model on entire data and use it to predict the test set\n",
    "    whole_ensemble = classifier.fit(x, y)\n",
    "    \n",
    "    #Predict for test data set and export test data set \n",
    "    test_disbursed = whole_ensemble.predict_proba(x_test)\n",
    "    \n",
    "    return test_disbursed[:,1]\n",
    "\n",
    "# approach 1\n",
    "# TODO - parameter tuning\n",
    "model1 = xgb.XGBClassifier(max_depth=3, n_estimators=700, learning_rate=0.05)\n",
    "data1, test1 = dataPreprocessing(data.copy(deep=True), test.copy(deep=True))\n",
    "solution_best1 = modeling(data1, test1, model1) # \n",
    "\n",
    "test1['Disbursed'] = solution_best1\n",
    "test1.to_csv('Solution_xgb.csv', columns=['Disbursed'],index=True)\n",
    "\n",
    "# TODO ensembling\n",
    "## approach 2\n",
    "#model2 = RandomForestClassifier(n_estimators=700)\n",
    "#data2, test2 = dataPreprocessing2(data.copy(deep=True), test.copy(deep=True))\n",
    "#solution_best2 = modeling(data2, test2, model2)\n",
    "#\n",
    "#test2['Disbursed'] = solution_best2\n",
    "#test2.to_csv('Solution2.csv', columns=['Disbursed'],index=True)\n",
    "\n",
    "## approach 3\n",
    "#model3 = GradientBoostingClassifier(n_estimators=700)\n",
    "#data3, test3 = dataPreprocessing1(data.copy(deep=True), test.copy(deep=True))\n",
    "#solution_best3 = modeling(data3, test3, model3)\n",
    "#\n",
    "#test3['Disbursed'] = solution_best3\n",
    "#test3.to_csv('Solution_gbm.csv', columns=['Disbursed'],index=True)\n",
    "#\n",
    "## check the correlation between the 2 approaches\n",
    "#plt.scatter(solution_best1,solution_best3)\n",
    "#plt.show()\n",
    "#print('Correlation between the 2 approaches = ', np.corrcoef(solution_best1,solution_best2)[0][1])\n",
    "#\n",
    "#glob_files = \"Solution*.csv\"\n",
    "#loc_outfile = \"rankedavg.csv\"\n",
    "#\n",
    "#def rankavg_ensemble(glob_files, loc_outfile):\n",
    "#    with open(loc_outfile,\"w\") as outfile:\n",
    "#        all_ranks = defaultdict(list)\n",
    "#        for i, glob_file in enumerate( glob(glob_files) ):\n",
    "#            file_ranks = []\n",
    "#            print(\"parsing:\", glob_file)\n",
    "#            for e, line in enumerate( open(glob_file) ):\n",
    "#                if e == 0 and i == 0:\n",
    "#                    outfile.write( line )\n",
    "#                elif e > 0:\n",
    "#                    r = line.strip().split(\",\")\n",
    "#                    file_ranks.append( (float(r[1]), e, r[0]) )\n",
    "#            for rank, item in enumerate( sorted(file_ranks) ):\n",
    "#                all_ranks[(item[1],item[2])].append(rank)\n",
    "#        average_ranks = []\n",
    "#        for k in sorted(all_ranks):\n",
    "#            average_ranks.append((sum(all_ranks[k])/len(all_ranks[k]),k))\n",
    "#        ranked_ranks = []\n",
    "#        for rank, k in enumerate(sorted(average_ranks)):\n",
    "#            ranked_ranks.append((k[1][0],k[1][1],rank/(len(average_ranks)-1)))\n",
    "#        for k in sorted(ranked_ranks):\n",
    "#            outfile.write(\"%s,%s\\n\"%(k[1],k[2]))\n",
    "#        print(\"wrote to %s\"%loc_outfile)\n",
    "#\n",
    "#rankavg_ensemble(glob_files, loc_outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
